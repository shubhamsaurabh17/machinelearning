# -*- coding: utf-8 -*-
"""Bank_customer_churn_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bo0f8vM19Axkoe0Ml3DMAnPtfKg8ychs

### **Importing Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Necessary Libraries
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import plotly.offline as po
import plotly.graph_objs as go
# %matplotlib inline

"""### **Reading Dataset**"""

# Importing the dataset
churn_data=pd.read_csv('churn_prediction.csv')
churn_data.head()

"""### **Data Exploration**"""

# Viewing some relevant information about data
churn_data.describe()

#Finding total no.of rows and columns in a dataset
churn_data.shape

#Finding correlation between different features of the data.It helps us in understanding the data well.
churn_data.corr()

# dropping out duplicate data ,if any
churn_data.drop_duplicates()
churn_data.shape

# Finding the data type of all columns
churn_data.dtypes

# Finding missing values, if any, in the dataset
churn_data.isnull().sum()

"""### **Imputing Missing values**"""

# Filling missing values of 'gender' column 
churn_data['gender'].fillna(churn_data['gender'].mode()[0],inplace=True)

# Filling missing values of "dependents" column
churn_data['dependents'].fillna(value=churn_data['dependents'].mode()[0],inplace=True)

# Filling missing values of "days_since_last_transaction" column
churn_data['days_since_last_transaction'].fillna(value=churn_data['days_since_last_transaction'].mean(),inplace=True)

# Filling missing values of "occupation" column
churn_data['occupation'].fillna(value=churn_data['occupation'].mode()[0],inplace=True)

# checking missing values again
churn_data.isnull().sum()

"""Here I am not imputing missing values of city column ,because it is very negatively correlated with the churn ,so I will drop this column.

### **Visualization**
"""

# Visualize Total Customer Churn
plot_by_churn_labels = churn_data["churn"].value_counts().keys().tolist()
plot_by_churn_values = churn_data["churn"].value_counts().values.tolist()

plot_data= [
    go.Pie(labels = plot_by_churn_labels,
           values = plot_by_churn_values,
           marker = dict(colors =  [ 'Teal' ,'Grey'],
                         line = dict(color = "white",
                                     width =  1.5)),
           rotation = 90,
           hoverinfo = "label+value+text",
           hole = .6)
]
plot_layout = go.Layout(dict(title = "Customer Churn",
                   plot_bgcolor  = "rgb(243,243,243)",
                   paper_bgcolor = "rgb(243,243,243)",))


fig = go.Figure(data=plot_data, layout=plot_layout)
po.iplot(fig)

# Visualize Churn Rate by Gender
plot_by_gender = churn_data.groupby('gender').churn.mean().reset_index()
plot_data = [
    go.Bar(
        x=plot_by_gender['gender'],
        y=plot_by_gender['churn'],
        width = [0.3, 0.3],
        marker=dict(
        color=['blue', 'orange'])
    )
]
plot_layout = go.Layout(
        xaxis={"type": "category"},
        yaxis={"title": "Churn Rate"},
        title='Churn Rate by Gender',
        plot_bgcolor  = 'rgb(243,243,243)',
        paper_bgcolor  = 'rgb(243,243,243)',
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
po.iplot(fig)

# Visualize Churn Rate by Occupation
plot_by_gender = churn_data.groupby('occupation').churn.mean().reset_index()
plot_data = [
    go.Bar(
        x=plot_by_gender['occupation'],
        y=plot_by_gender['churn'],
        width = [0.3, 0.3,0.3,0.3,0.3],
        marker=dict(
        color=['blue', 'orange','green','brown','red'])
    )
]
plot_layout = go.Layout(
        xaxis={"type": "category"},
        yaxis={"title": "Churn Rate"},
        title='Churn Rate by Occupation',
        plot_bgcolor  = 'rgb(243,243,243)',
        paper_bgcolor  = 'rgb(243,243,243)',
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
po.iplot(fig)

# # Visualize Churn Rate by customer net worth category
plot_by_gender = churn_data.groupby('customer_nw_category').churn.mean().reset_index()
plot_data = [
    go.Bar(
        x=plot_by_gender['customer_nw_category'],
        y=plot_by_gender['churn'],
        width = [0.3, 0.3, 0.3],
        marker=dict(
        color=['blue', 'orange','green'])
    )
]
plot_layout = go.Layout(
        xaxis={"type": "category"},
        yaxis={"title": "Churn Rate"},
        title='Churn Rate by customer_nw_category',
        plot_bgcolor  = 'rgb(243,243,243)',
        paper_bgcolor  = 'rgb(243,243,243)',
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
po.iplot(fig)

"""### **Encoding categorical variable**"""

# Encoding categorical variable 'gender' and 'occupation'
churn_data=pd.get_dummies(churn_data,columns=['gender','occupation'],drop_first=True)

# After encoding ,no.of columns got increased
churn_data.columns

"""### **Feature Scaling**"""

#performing feature scaling
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
columns_fit_for_scaling=['vintage', 'age', 'dependents','customer_nw_category', 'branch_code', 'days_since_last_transaction',
       'current_balance', 'previous_month_end_balance','average_monthly_balance_prevQ', 'average_monthly_balance_prevQ2',
       'current_month_credit', 'previous_month_credit', 'current_month_debit','previous_month_debit', 'current_month_balance',
       'previous_month_balance']
churn_data[columns_fit_for_scaling]=sc.fit_transform(churn_data[columns_fit_for_scaling])

# seeing subset of data after feature scaling
churn_data.head()

"""### **Separation of training and testing data**"""

# separating dependent and independent variables
X=churn_data.drop(['churn','customer_id','city'],axis=1)
Y=churn_data['churn']

"""### **splitting of dataset**"""

# splitting the data into training set(70%) and test set(30%)
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 50)

"""### **Model building, training and prediction**"""

# Machine Learning classification model libraries
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

#Fit the logistic Regression Model
lmodel = LogisticRegression(random_state=50)
lmodel.fit(X_train,Y_train)

#Predict the value for new, unseen data
pred = lmodel.predict(X_test)

# Find Accuracy using accuracy_score method
lmodel_accuracy = round(metrics.accuracy_score(Y_test, pred) * 100, 2)

#Fit the K-Nearest Neighbor Model
from sklearn.neighbors import KNeighborsClassifier
knnmodel = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2) #p=2 represents Euclidean distance, p=1 represents Manhattan Distance
knnmodel.fit(X_train, Y_train) 
  
#Predict the value for new, unseen data
knn_pred = knnmodel.predict(X_test)

# Find Accuracy using accuracy_score method
knn_accuracy = round(metrics.accuracy_score(Y_test, knn_pred) * 100, 2)

#Fit the Decision Tree Classification Model
from sklearn.tree import DecisionTreeClassifier
dtmodel = DecisionTreeClassifier(criterion = "gini", random_state = 50)
dtmodel.fit(X_train, Y_train) 
  
#Predict the value for new, unseen data
dt_pred = dtmodel.predict(X_test)

# Find Accuracy using accuracy_score method
dt_accuracy = round(metrics.accuracy_score(Y_test, dt_pred) * 100, 2)

#Fit the Random Forest Classification Model
from sklearn.ensemble import RandomForestClassifier
rfmodel = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 50)
rfmodel.fit(X_train, Y_train) 
  
#Predict the value for new, unseen data
rf_pred = rfmodel.predict(X_test)

# Find Accuracy using accuracy_score method
rf_accuracy = round(metrics.accuracy_score(Y_test, rf_pred) * 100, 2)

# Compare Several models according to their Accuracies
M_Comp = pd.DataFrame({
    'Model': ['Logistic Regression', 'K-Nearest Neighbor', 
              'Decision Tree', 'Random Forest'],
    'Score': [lmodel_accuracy, knn_accuracy, 
              dt_accuracy, rf_accuracy]})
M_Comp_df = M_Comp.sort_values(by='Score', ascending=False)
M_Comp_df = M_Comp_df.set_index('Score')
M_Comp_df.reset_index()

#Generate confusion matrix for Random Forest model as it has maximum Accuracy
from sklearn.metrics import confusion_matrix
conf_mat_rfmodel = confusion_matrix(Y_test,pred)
conf_mat_rfmodel

# Predict the probability of Churn of each customer
churn_data['Probability_of_Churn'] = rfmodel.predict_proba(churn_data[X_test.columns])[:,1]

# Create a Dataframe showcasing probability of Churn of each customer
churn_data[['customer_id','Probability_of_Churn']].head()